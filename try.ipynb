{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/lclan/anaconda3/envs/rob_sac/lib/python3.7/site-packages/gym']\n",
      "(111,)\n",
      "(15,)\n",
      "(14,)\n",
      "(15,)\n",
      "(14,)\n",
      "(111,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lclan/anaconda3/envs/rob_sac/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from copy import deepcopy\n",
    "print(gym.__path__)\n",
    "env = gym.make('Ant-v3')\n",
    "o = env.reset()\n",
    "old_state = deepcopy(env.sim.get_state())\n",
    "\n",
    "def setEnvWithState(env, state):\n",
    "    env.sim.set_state(state)\n",
    "    env.sim.forward()\n",
    "    return env.get_obs()\n",
    "\n",
    "def setEnvWithObs(env, obs):\n",
    "    state = deepcopy(env.sim.get_state())\n",
    "    state.qpos = obs[:state.qpos.shape[0]]\n",
    "    state.qvel = obs[state.qpos.shape[0]:state.qpos.shape[0]+state.qvel.shape[0]]\n",
    "    env.sim.set_state(state)\n",
    "    env.sim.forward()\n",
    "    return env.get_obs()\n",
    "\n",
    "\n",
    "print(o.shape)\n",
    "print(o[:old_state.qpos.shape[0]].shape)\n",
    "print(o[old_state.qpos.shape[0]:old_state.qpos.shape[0]+old_state.qvel.shape[0]].shape)\n",
    "print(old_state.qpos.flat.copy().shape)\n",
    "print(old_state.qvel.flat.copy().shape)\n",
    "print(env.get_obs().shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "import torch\n",
    "import os.path as osp\n",
    "def test_model(env, model, max_ep_len=None, num_episodes=20, interval = 200):\n",
    "    o, r, d, ep_ret, ep_len, n = env.reset(), 0, False, 0, 0, 0\n",
    "    total_rewards = []\n",
    "    trajs = []\n",
    "    mid_points = []\n",
    "    while n < num_episodes:\n",
    "        old_state = deepcopy(env.sim.get_state())\n",
    "        mid_points.append((ep_len, old_state, o))\n",
    "        a = get_action(o, model)\n",
    "        o, r, d, _ = env.step(a)\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            total_rewards.append(ep_ret)\n",
    "            trajs.append(mid_points)\n",
    "            mid_points = [] # clear mid point\n",
    "            o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "            n += 1\n",
    "    return total_rewards, trajs\n",
    "\n",
    "def get_models(path, env_name, name):\n",
    "    env = gym.make(env_name)\n",
    "    fpath = osp.join(path, name)\n",
    "    print(fpath)\n",
    "    models = []\n",
    "    while 1:\n",
    "        fname = osp.join(fpath, name + \"_s\" + str(len(models)) ,'pyt_save', 'model.pt')\n",
    "        print(fname)\n",
    "        if(osp.exists(fname)):\n",
    "            model = torch.load(fname)\n",
    "            models.append(model)\n",
    "        else:\n",
    "            break\n",
    "    # print(len(models))\n",
    "    test_val = []\n",
    "    env = gym.make(env_name)\n",
    "    for model in models:\n",
    "        x, _ = test_model(env, model, num_episodes=5)\n",
    "        # print(x, stats.trim_mean(x, 0.1))\n",
    "        test_val.append(stats.trim_mean(x, 0.1))\n",
    "    sorted_ids = np.argsort(test_val)\n",
    "    model1 = models[sorted_ids[-1]]\n",
    "    model2 = models[sorted_ids[-2]]\n",
    "    return model1, model2\n",
    "\n",
    "def get_action(o, md):\n",
    "    with torch.no_grad():\n",
    "        o = torch.as_tensor(o, dtype=torch.float32)\n",
    "        a = md.act(o)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lclan/anaconda3/envs/rob_sac/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lclan/spinningup/data/Ant-v3_sac_base\n",
      "/home/lclan/spinningup/data/Ant-v3_sac_base/Ant-v3_sac_base_s0/pyt_save/model.pt\n",
      "/home/lclan/spinningup/data/Ant-v3_sac_base/Ant-v3_sac_base_s1/pyt_save/model.pt\n",
      "/home/lclan/spinningup/data/Ant-v3_sac_base/Ant-v3_sac_base_s2/pyt_save/model.pt\n",
      "/home/lclan/spinningup/data/Ant-v3_sac_base/Ant-v3_sac_base_s3/pyt_save/model.pt\n",
      "/home/lclan/spinningup/data/Ant-v3_sac_base/Ant-v3_sac_base_s4/pyt_save/model.pt\n"
     ]
    }
   ],
   "source": [
    "m1, m2 = get_models('/home/lclan/spinningup/data/', 'Ant-v3', 'Ant-v3_sac_base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366.19022\n",
      "False\n",
      "366.025\n",
      "-19.158342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lclan/anaconda3/envs/rob_sac/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "def get_value(o, md):\n",
    "    # with torch.no_grad():\n",
    "    a, _ = md.pi(o, deterministic=True, with_logprob=False)\n",
    "    q1 = md.q1(o, a)\n",
    "    q2 = md.q2(o, a)\n",
    "    q = torch.min(q1, q2)\n",
    "    return q\n",
    "\n",
    "def set_state_with_obs(env, obs):\n",
    "    if(torch.is_tensor(obs)):\n",
    "        obs = obs.detach().numpy()\n",
    "    nq = env.model.nq\n",
    "    nv = env.model.nv\n",
    "    qpos = obs[:nq]\n",
    "    qvel = obs[nq:nq+nv]\n",
    "    env.set_state(qpos, qvel)\n",
    "    new_obs = env.get_obs()\n",
    "    return new_obs\n",
    "\n",
    "def save_state(env):\n",
    "    qpos = deepcopy(env.sim.data.qpos)\n",
    "    qvel = deepcopy(env.sim.data.qvel)\n",
    "    return (qpos ,qvel)\n",
    "\n",
    "def restore_state(env, old_state):\n",
    "    env.set_state(old_state[0], old_state[1])\n",
    "    new_obs = env.get_obs()\n",
    "    return new_obs\n",
    "\n",
    "max_eps_t = 0.0001\n",
    "env = gym.make('Ant-v3')\n",
    "o = env.reset()\n",
    "old_state = save_state(env)\n",
    "\n",
    "with torch.torch.no_grad():\n",
    "    oo2 = torch.as_tensor(o, dtype=torch.float32)\n",
    "    q2 = get_value(oo2, m1)\n",
    "    print(q2.detach().numpy())\n",
    "\n",
    "\n",
    "with torch.set_grad_enabled(True):\n",
    "    oo = torch.as_tensor(o, dtype=torch.float32)\n",
    "    print(oo.requires_grad)\n",
    "    oo.requires_grad = True\n",
    "    q = get_value(oo, m1)\n",
    "    grad_o = torch.autograd.grad(q, [oo], retain_graph=True)[0]\n",
    "\n",
    "\n",
    "o_adv = oo - max_eps_t * grad_o.detach().sign()\n",
    "\n",
    "with torch.torch.no_grad():\n",
    "    oo2 = torch.as_tensor(o_adv, dtype=torch.float32)\n",
    "    q2 = get_value(oo2, m1)\n",
    "    print(q2.detach().numpy())\n",
    "\n",
    "new_obs = set_state_with_obs(env, o_adv)\n",
    "\n",
    "with torch.torch.no_grad():\n",
    "    oo2 = torch.as_tensor(new_obs, dtype=torch.float32)\n",
    "    q2 = get_value(oo2, m1)\n",
    "    print(q2.detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.192668482775532\n",
      "1155.4254482090028\n"
     ]
    }
   ],
   "source": [
    "def run_extra_steps(env, o, ep_len, md, max_ep_len, step_num = 50):\n",
    "    # return 0\n",
    "    # print(env.done)\n",
    "    total_r = 0\n",
    "    for i in range(step_num):\n",
    "        a = get_action(o, md)\n",
    "        o, r, d, _ = env.step(a)\n",
    "        total_r += r\n",
    "        ep_len += 1\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            # print(i, d, r, ep_len, max_ep_len)\n",
    "            return (d, total_r)\n",
    "    return (d, total_r)\n",
    "\n",
    "d, r = run_extra_steps(env, new_obs, 0, m1, 1000, 200)\n",
    "print(r)\n",
    "new_obs = restore_state(env, old_state)\n",
    "d, r = run_extra_steps(env, new_obs, 0, m1, 1000, 200)\n",
    "print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383.5152\n",
      "-8.68798\n"
     ]
    }
   ],
   "source": [
    "def get_value(o, md):\n",
    "    if not torch.is_tensor(o):\n",
    "        o = torch.as_tensor(o, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        a, _ = md.pi(o, deterministic=True, with_logprob=False)\n",
    "        q1 = md.q1(o, a)\n",
    "        q2 = md.q2(o, a)\n",
    "        q = torch.min(q1, q2)\n",
    "    return q\n",
    "\n",
    "def get_grad(env, md, eps=0.001):\n",
    "    qpos = deepcopy(env.sim.data.qpos)\n",
    "    qvel = deepcopy(env.sim.data.qvel)\n",
    "    o = env.get_obs()\n",
    "    old_q = get_value(o, md).numpy()\n",
    "    grad_pos = np.zeros_like(qpos)\n",
    "    for i in range(len(qpos)):\n",
    "        qpos[i]+=eps\n",
    "        env.set_state(qpos, qvel)\n",
    "        o = env.get_obs()\n",
    "        q = get_value(o, md).numpy()\n",
    "        grad_pos[i] = (q - old_q) / eps        \n",
    "        qpos[i]-=eps\n",
    "        \n",
    "    grad_vel = np.zeros_like(qvel)\n",
    "    for i in range(len(qvel)):\n",
    "        qvel[i]+=eps\n",
    "        env.set_state(qpos, qvel)\n",
    "        o = env.get_obs()\n",
    "        q = get_value(o, md).numpy()\n",
    "        grad_vel[i] = (q - old_q) / eps        \n",
    "        qvel[i]-=eps\n",
    "    return grad_pos, grad_vel\n",
    "\n",
    "def change_env_to_adv(env, md, max_eps_t):\n",
    "    qpos = deepcopy(env.sim.data.qpos)\n",
    "    qvel = deepcopy(env.sim.data.qvel)\n",
    "    grad_pos, grad_vel = get_grad(env, md, max_eps_t)\n",
    "    # grad_pos = np.clip(grad_pos, -1, 1)\n",
    "    # grad_vel = np.clip(grad_vel, -1, 1)\n",
    "    adv_qpos = qpos - max_eps_t*grad_pos\n",
    "    adv_qvel = qvel - max_eps_t*grad_vel\n",
    "    env.set_state(adv_qpos, adv_qvel)\n",
    "    new_obs = env.get_obs()\n",
    "    return new_obs\n",
    "\n",
    "max_eps_t = 0.01\n",
    "env = gym.make('Ant-v3')\n",
    "o = env.reset()\n",
    "old_state = save_state(env)\n",
    "with torch.torch.no_grad():\n",
    "    oo2 = torch.as_tensor(o, dtype=torch.float32)\n",
    "    q2 = get_value(oo2, m1)\n",
    "    print(q2.detach().numpy())\n",
    "\n",
    "o = change_env_to_adv(env, m1, max_eps_t)\n",
    "with torch.torch.no_grad():\n",
    "    oo2 = torch.as_tensor(o, dtype=torch.float32)\n",
    "    q2 = get_value(oo2, m1)\n",
    "    print(q2.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spinup.algos.pytorch.sac.core as core\n",
    "from torch.optim import Adam\n",
    "import itertools\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for SAC agents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(obs=self.obs_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     act=self.act_buf[idxs],\n",
    "                     rew=self.rew_buf[idxs],\n",
    "                     done=self.done_buf[idxs])\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n"
     ]
    }
   ],
   "source": [
    "def get_action(ac, o, deterministic=False):\n",
    "    return ac.act(torch.as_tensor(o, dtype=torch.float32), \n",
    "                    deterministic)\n",
    "def get_replay_buff(ac, replay_size = int(1e6)):\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    max_ep_len = 100\n",
    "    rs = []\n",
    "    for t in range(replay_size):\n",
    "        if t % 100000 == 0:\n",
    "            print(t)\n",
    "        a = get_action(ac, o, deterministic=False)\n",
    "        o2, r, d, _ = env.step(a)\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "        d = False if ep_len==max_ep_len else d\n",
    "        replay_buffer.store(o, a, r, o2, d)\n",
    "        o = o2\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            rs.append(ep_ret)\n",
    "            o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    \n",
    "    return replay_buffer\n",
    "\n",
    "replay_buffer = get_replay_buff(m1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_q(ac, ac_targ, data):\n",
    "    gamma = 0.99\n",
    "    o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
    "    q1 = ac.q1(o,a)\n",
    "    q2 = ac.q2(o,a)\n",
    "    with torch.no_grad():\n",
    "        a2, _ = ac.pi(o2)\n",
    "        q1_pi_targ = ac_targ.q1(o2, a2)\n",
    "        q2_pi_targ = ac_targ.q2(o2, a2)\n",
    "        q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)\n",
    "        backup = r + gamma * (1 - d) * (q_pi_targ)\n",
    "    loss_q1 = ((q1 - backup)**2).mean()\n",
    "    loss_q2 = ((q2 - backup)**2).mean()\n",
    "    loss_q = loss_q1 + loss_q2\n",
    "    return loss_q\n",
    "\n",
    "def train_with_buffer(ac, replay_buffer):\n",
    "    ac_targ = deepcopy(ac)\n",
    "    polyak=0.995\n",
    "    for p in ac_targ.parameters():\n",
    "        p.requires_grad = False\n",
    "    q_params = itertools.chain(ac.q1.parameters(), ac.q2.parameters())\n",
    "    q_optimizer = Adam(q_params, lr=1e-3)\n",
    "    batch_size = 32\n",
    "    total_steps = (int)(replay_buffer.max_size / batch_size)\n",
    "    for t in range(total_steps):\n",
    "        batch = replay_buffer.sample_batch(batch_size)\n",
    "        q_optimizer.zero_grad()\n",
    "        loss_q = compute_loss_q(ac, ac_targ, batch)\n",
    "        loss_q.backward()\n",
    "        q_optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "                # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "                # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "                p_targ.data.mul_(polyak)\n",
    "                p_targ.data.add_((1 - polyak) * p.data)\n",
    "\n",
    "md = deepcopy(m1)\n",
    "train_with_buffer(md, replay_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5700.78 5146.02 379.25 224.51\n",
      "5644.68 3912.45 368.31 3.62\n"
     ]
    }
   ],
   "source": [
    "def print_2f(*args):\n",
    "    __builtins__.print(*(\"%.2f\" % a if isinstance(a, float) else a\n",
    "                         for a in args))\n",
    "\n",
    "def get_action(o, md):\n",
    "    with torch.no_grad():\n",
    "        o = torch.as_tensor(o, dtype=torch.float32)\n",
    "        a = md.act(o)\n",
    "    return a\n",
    "\n",
    "def run_extra_steps(env, o, ep_len, md, max_ep_len, step_num = 50):\n",
    "    total_r = 0\n",
    "    for i in range(step_num):\n",
    "        a = get_action(o, md)\n",
    "        o, r, d, _ = env.step(a)\n",
    "        total_r += r\n",
    "        ep_len += 1\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            # print(i, d, r, ep_len, max_ep_len)\n",
    "            return (d, total_r)\n",
    "    return (d, total_r)\n",
    "\n",
    "def get_value(o, md):\n",
    "    if not torch.is_tensor(o):\n",
    "        o = torch.as_tensor(o, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        a, _ = md.pi(o, deterministic=True, with_logprob=False)\n",
    "        q1 = md.q1(o, a)\n",
    "        q2 = md.q2(o, a)\n",
    "        q = torch.min(q1, q2)\n",
    "    return q\n",
    "\n",
    "def get_grad(env, md, eps=0.01):\n",
    "    qpos = deepcopy(env.sim.data.qpos)\n",
    "    qvel = deepcopy(env.sim.data.qvel)\n",
    "    o = env.get_obs()\n",
    "    old_q = get_value(o, md).numpy()\n",
    "    grad_pos = np.zeros_like(qpos)\n",
    "    for i in range(len(qpos)):\n",
    "        qpos[i]+=eps\n",
    "        env.set_state(qpos, qvel)\n",
    "        o = env.get_obs()\n",
    "        q = get_value(o, md).numpy()\n",
    "        grad_pos[i] = (q - old_q) / eps        \n",
    "        qpos[i]-=eps\n",
    "        \n",
    "    grad_vel = np.zeros_like(qvel)\n",
    "    for i in range(len(qvel)):\n",
    "        qvel[i]+=eps\n",
    "        env.set_state(qpos, qvel)\n",
    "        o = env.get_obs()\n",
    "        q = get_value(o, md).numpy()\n",
    "        grad_vel[i] = (q - old_q) / eps        \n",
    "        qvel[i]-=eps\n",
    "    return grad_pos, grad_vel\n",
    "\n",
    "def change_env_to_adv(env, md, max_eps_t):\n",
    "    qpos = deepcopy(env.sim.data.qpos)\n",
    "    qvel = deepcopy(env.sim.data.qvel)\n",
    "    grad_pos, grad_vel = get_grad(env, md, max_eps_t*0.1)\n",
    "    grad_pos = np.clip(grad_pos, -1, 1)\n",
    "    grad_vel = np.clip(grad_vel, -1, 1)\n",
    "    # adv_qpos = qpos + np.random.uniform(\n",
    "    #         low=-max_eps_t, high=+max_eps_t, size=env.model.nq)\n",
    "    # adv_qvel = qvel + np.random.uniform(\n",
    "    #         low=-max_eps_t, high=+max_eps_t, size=env.model.nv)\n",
    "    adv_qpos = qpos - max_eps_t*grad_pos\n",
    "    adv_qvel = qvel - max_eps_t*grad_vel\n",
    "    init_qpos = env.sim.data.qpos.ravel().copy()\n",
    "    init_qvel = env.sim.data.qvel.ravel().copy()\n",
    "    # rns = env._reset_noise_scale\n",
    "    rns = 0.1\n",
    "    adv_qpos = np.clip(adv_qpos, init_qpos - rns,  init_qpos + rns)\n",
    "    adv_qvel = np.clip(adv_qvel, init_qvel - 2*rns, init_qvel + 2*rns)\n",
    "    \n",
    "    env.set_state(adv_qpos, adv_qvel)\n",
    "    new_obs = env.get_obs()\n",
    "    return new_obs\n",
    "\n",
    "def save_state(env):\n",
    "    qpos = deepcopy(env.sim.data.qpos)\n",
    "    qvel = deepcopy(env.sim.data.qvel)\n",
    "    return (qpos ,qvel)\n",
    "\n",
    "def restore_state(env, old_state):\n",
    "    env.reset()\n",
    "    env.set_state(old_state[0], old_state[1])\n",
    "    new_obs = env.get_obs()\n",
    "    return new_obs\n",
    "\n",
    "def get_mean(v):\n",
    "    return sum(v)/len(v)\n",
    "\n",
    "def testing(md, env_name, max_eps_t = 0.01):\n",
    "    test_num = 50\n",
    "    test_l = 1000\n",
    "    env = gym.make(env_name)\n",
    "    orig_rs = []\n",
    "    adv_rs = []\n",
    "    orig_qs = []\n",
    "    adv_qs = []\n",
    "    for i in range(test_num):\n",
    "        o = env.reset()\n",
    "        old_state = save_state(env)\n",
    "        with torch.torch.no_grad():\n",
    "            q = get_value(o, md).numpy()\n",
    "        orig_qs.append(q)\n",
    "        _, r = run_extra_steps(env, o, 0, md, 1000, test_l)\n",
    "        orig_rs.append(r)\n",
    "        \n",
    "        restore_state(env, old_state)\n",
    "        for _ in range(20):\n",
    "            o = change_env_to_adv(env, md, max_eps_t)\n",
    "\n",
    "        with torch.torch.no_grad():\n",
    "            q = get_value(o, md).numpy()\n",
    "        adv_qs.append(q)\n",
    "        _, r = run_extra_steps(env, o, 0, md, 1000, test_l)\n",
    "        adv_rs.append(r)\n",
    "    print_2f(get_mean(orig_rs), get_mean(adv_rs), get_mean(orig_qs), get_mean(adv_qs))\n",
    "\n",
    "testing(m1, 'Ant-v3')\n",
    "testing(md, 'Ant-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(ac, o, deterministic=False):\n",
    "    return ac.act(torch.as_tensor(o, dtype=torch.float32), \n",
    "                    deterministic)\n",
    "def get_adv_replay_buff(ac, replay_size = int(1e6)):\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    max_ep_len = 1000\n",
    "    rs = []\n",
    "    use_adv = False\n",
    "    for t in range(replay_size):\n",
    "        if t % 100000 == 0:\n",
    "            print(t)\n",
    "        a = get_action(ac, o, deterministic=False)\n",
    "        o2, r, d, _ = env.step(a)\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "        d = False if ep_len==max_ep_len else d\n",
    "        replay_buffer.store(o, a, r, o2, d)\n",
    "        o = o2\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            rs.append(ep_ret)\n",
    "            o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "            if(use_adv):\n",
    "                use_adv = False\n",
    "                for _ in range(10):\n",
    "                    o = change_env_to_adv(env, md, max_eps_t)\n",
    "            else:\n",
    "                use_adv = True\n",
    "    \n",
    "    return replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n"
     ]
    }
   ],
   "source": [
    "replay_buffer2 =  get_adv_replay_buff(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_q(ac, ac_targ, data):\n",
    "    gamma = 0.99\n",
    "    o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
    "    q1 = ac.q1(o,a)\n",
    "    q2 = ac.q2(o,a)\n",
    "    with torch.no_grad():\n",
    "        a2, _ = ac.pi(o2)\n",
    "        q1_pi_targ = ac_targ.q1(o2, a2)\n",
    "        q2_pi_targ = ac_targ.q2(o2, a2)\n",
    "        q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)\n",
    "        backup = r + gamma * (1 - d) * (q_pi_targ)\n",
    "    loss_q1 = ((q1 - backup)**2).mean()\n",
    "    loss_q2 = ((q2 - backup)**2).mean()\n",
    "    loss_q = loss_q1 + loss_q2\n",
    "    return loss_q\n",
    "\n",
    "def train_with_buffer(ac, replay_buffer):\n",
    "    ac_targ = deepcopy(ac)\n",
    "    polyak=0.995\n",
    "    for p in ac_targ.parameters():\n",
    "        p.requires_grad = False\n",
    "    q_params = itertools.chain(ac.q1.parameters(), ac.q2.parameters())\n",
    "    q_optimizer = Adam(q_params, lr=1e-3)\n",
    "    batch_size = 32\n",
    "    total_steps = (int)(replay_buffer.max_size / batch_size)\n",
    "    for t in range(total_steps):\n",
    "        batch = replay_buffer.sample_batch(batch_size)\n",
    "        q_optimizer.zero_grad()\n",
    "        loss_q = compute_loss_q(ac, ac_targ, batch)\n",
    "        loss_q.backward()\n",
    "        q_optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "                # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "                # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "                p_targ.data.mul_(polyak)\n",
    "                p_targ.data.add_((1 - polyak) * p.data)\n",
    "\n",
    "md2 = deepcopy(md)\n",
    "train_with_buffer(md2, replay_buffer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lclan/anaconda3/envs/rob_sac/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5706.19 5588.91 373.61 316.58\n",
      "10 5813.85 5820.34 367.53 263.14\n",
      "10 5683.75 5962.37 368.95 301.56\n",
      "20 5849.45 4411.02 376.55 187.66\n",
      "20 5773.06 4970.68 366.53 36.66\n",
      "20 5663.79 2920.11 366.33 41.21\n",
      "40 5568.68 2379.38 373.70 -29.58\n",
      "40 5872.23 936.78 367.03 -227.79\n",
      "40 5860.24 313.66 368.74 -317.25\n",
      "80 5830.76 458.36 376.87 -418.50\n",
      "80 5665.79 -11.66 369.78 -486.98\n",
      "80 5825.31 -5.85 367.49 -712.43\n"
     ]
    }
   ],
   "source": [
    "def get_action(o, md):\n",
    "    with torch.no_grad():\n",
    "        o = torch.as_tensor(o, dtype=torch.float32)\n",
    "        a = md.act(o)\n",
    "    return a\n",
    "\n",
    "def testing(md, env_name, adv_step, max_eps_t = 0.01):\n",
    "    test_num = 50\n",
    "    test_l = 1000\n",
    "    env = gym.make(env_name)\n",
    "    orig_rs = []\n",
    "    adv_rs = []\n",
    "    orig_qs = []\n",
    "    adv_qs = []\n",
    "    for i in range(test_num):\n",
    "        o = env.reset()\n",
    "        old_state = save_state(env)\n",
    "        with torch.torch.no_grad():\n",
    "            q = get_value(o, md).numpy()\n",
    "        orig_qs.append(q)\n",
    "        _, r = run_extra_steps(env, o, 0, md, 1000, test_l)\n",
    "        orig_rs.append(r)\n",
    "        \n",
    "        restore_state(env, old_state)\n",
    "        for _ in range(adv_step):\n",
    "            o = change_env_to_adv(env, md, max_eps_t)\n",
    "\n",
    "        with torch.torch.no_grad():\n",
    "            q = get_value(o, md).numpy()\n",
    "        adv_qs.append(q)\n",
    "        _, r = run_extra_steps(env, o, 0, md, 1000, test_l)\n",
    "        adv_rs.append(r)\n",
    "    print_2f(adv_step, get_mean(orig_rs), get_mean(adv_rs), get_mean(orig_qs), get_mean(adv_qs))\n",
    "\n",
    "testing(m1, 'Ant-v3', 10)\n",
    "testing(md, 'Ant-v3', 10)\n",
    "testing(md2, 'Ant-v3', 10)\n",
    "testing(m1, 'Ant-v3', 20)\n",
    "testing(md, 'Ant-v3', 20)\n",
    "testing(md2, 'Ant-v3', 20)\n",
    "testing(m1, 'Ant-v3', 40)\n",
    "testing(md, 'Ant-v3', 40)\n",
    "testing(md2, 'Ant-v3', 40)\n",
    "testing(m1, 'Ant-v3', 80)\n",
    "testing(md, 'Ant-v3', 80)\n",
    "testing(md2, 'Ant-v3', 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(o, md):\n",
    "    with torch.no_grad():\n",
    "        o = torch.as_tensor(o, dtype=torch.float32)\n",
    "        a = md.act(o)\n",
    "    return a\n",
    "\n",
    "def get_state_dis(s1, s2):\n",
    "    l = len(s1[0]) + len(s1[1])\n",
    "    l1_mean_dis = (sum(np.abs(s1[0]-s2[0])) + sum(np.abs(s1[1]-s2[1]))) / l\n",
    "    l2_mean_dis = np.sqrt((sum((s1[0]-s2[0])**2) + sum((s1[1]-s2[1])**2))/l)\n",
    "    return l1_mean_dis, l2_mean_dis\n",
    "        \n",
    "def get_init_state(env):\n",
    "    env.sim.reset()\n",
    "    init_qpos = env.sim.data.qpos.ravel().copy()\n",
    "    init_qvel = env.sim.data.qvel.ravel().copy()\n",
    "    return (init_qpos, init_qvel)\n",
    "def print_2f(*args):\n",
    "    __builtins__.print(*(\"%.2f\" % a if isinstance(a, float) else a\n",
    "                         for a in args), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,3,2])\n",
    "b = np.array([2,4,1])\n",
    "print(sum(np.abs(a-b)))\n",
    "print(np.sqrt(sum((a-b)**2)/3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lclan/anaconda3/envs/rob_sac/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5721.09 5872.24 376.68 317.13\n",
      "dis:  0.11 0.14 0.11 0.13\n",
      "10 5973.47 5852.39 369.21 280.29\n",
      "dis:  0.11 0.14 0.12 0.14\n",
      "10 5892.71 5661.84 367.57 307.97\n",
      "dis:  0.11 0.14 0.12 0.14\n",
      "20 5733.54 5672.60 374.34 314.58\n",
      "dis:  0.11 0.14 0.12 0.13\n",
      "20 5750.34 5803.90 369.24 278.52\n",
      "dis:  0.11 0.15 0.12 0.14\n",
      "20 5922.85 5745.58 365.62 306.31\n",
      "dis:  0.11 0.14 0.12 0.14\n",
      "40 5560.87 5590.67 377.79 316.40\n",
      "dis:  0.11 0.14 0.12 0.13\n",
      "40 5581.64 5463.63 366.13 278.84\n",
      "dis:  0.11 0.14 0.12 0.14\n",
      "40 5677.23 5723.21 366.03 308.41\n",
      "dis:  0.11 0.14 0.12 0.14\n",
      "80 5806.91 5795.64 376.87 313.66\n",
      "dis:  0.11 0.14 0.12 0.13\n",
      "80 5889.10 5823.45 365.69 280.15\n",
      "dis:  0.11 0.14 0.12 0.14\n",
      "80 5838.21 5731.73 368.39 309.61\n",
      "dis:  0.11 0.14 0.12 0.14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def change_env_to_adv(env, md, max_eps_t, init_state):\n",
    "    qpos = deepcopy(env.sim.data.qpos)\n",
    "    qvel = deepcopy(env.sim.data.qvel)\n",
    "    grad_pos, grad_vel = get_grad(env, md, max_eps_t)\n",
    "    grad_pos = np.sign(grad_pos)\n",
    "    grad_vel = np.sign(grad_vel)\n",
    "    # grad_pos = np.clip(grad_pos, -1, 1)\n",
    "    # grad_vel = np.clip(grad_vel, -1, 1)\n",
    "    # adv_qpos = qpos + np.random.uniform(\n",
    "    #         low=-max_eps_t, high=+max_eps_t, size=env.model.nq)\n",
    "    # adv_qvel = qvel + np.random.uniform(\n",
    "    #         low=-max_eps_t, high=+max_eps_t, size=env.model.nv)\n",
    "    adv_qpos = qpos - max_eps_t*grad_pos\n",
    "    adv_qvel = qvel - 2.0* max_eps_t*grad_vel\n",
    "    init_qpos = init_state[0]\n",
    "    init_qvel = init_state[1]\n",
    "    # rns = env._reset_noise_scale\n",
    "    rns = 0.1\n",
    "    adv_qpos = np.clip(adv_qpos, init_qpos - rns,  init_qpos + rns)\n",
    "    adv_qvel = np.clip(adv_qvel, init_qvel - 2*rns, init_qvel + 2*rns)\n",
    "    \n",
    "    env.set_state(adv_qpos, adv_qvel)\n",
    "    new_obs = env.get_obs()\n",
    "    return new_obs\n",
    "\n",
    "def testing(md, env_name, adv_step, max_eps_t = 0.1):\n",
    "    test_num = 50\n",
    "    test_l = 1000\n",
    "    env = gym.make(env_name)\n",
    "    orig_rs = []\n",
    "    adv_rs = []\n",
    "    orig_qs = []\n",
    "    adv_qs = []\n",
    "    adv_step_size = 1.5 * max_eps_t / adv_step\n",
    "    l1_old, l2_old, l1_init, l2_init = [], [], [], []\n",
    "    init_state = get_init_state(env)\n",
    "    for i in range(test_num):\n",
    "        o = env.reset()\n",
    "        old_state = save_state(env)\n",
    "        with torch.torch.no_grad():\n",
    "            q = get_value(o, md).numpy()\n",
    "        orig_qs.append(q)\n",
    "        _, r = run_extra_steps(env, o, 0, md, 1000, test_l)\n",
    "        orig_rs.append(r)\n",
    "        \n",
    "        restore_state(env, old_state)\n",
    "        for _ in range(adv_step):\n",
    "            o = change_env_to_adv(env, md, adv_step_size, init_state)\n",
    "        new_state = save_state(env)\n",
    "        l1, l2 = get_state_dis(new_state, old_state)\n",
    "        l1_old.append(l1)\n",
    "        l2_old.append(l2)\n",
    "        l1, l2 = get_state_dis(new_state, init_state)\n",
    "        l1_init.append(l1)\n",
    "        l2_init.append(l2)\n",
    "        with torch.torch.no_grad():\n",
    "            q = get_value(o, md).numpy()\n",
    "        adv_qs.append(q)\n",
    "        _, r = run_extra_steps(env, o, 0, md, 1000, test_l)\n",
    "        adv_rs.append(r)\n",
    "    print_2f(adv_step, adv_step_size, get_mean(orig_rs), get_mean(adv_rs), get_mean(orig_qs), get_mean(adv_qs))\n",
    "    print_2f(\"dis: \", get_mean(l1_old), get_mean(l2_old), get_mean(l1_init), get_mean(l2_init))\n",
    "    \n",
    "\n",
    "testing(m1, 'Ant-v3', 10)\n",
    "testing(md, 'Ant-v3', 10)\n",
    "testing(md2, 'Ant-v3', 10)\n",
    "testing(m1, 'Ant-v3', 20)\n",
    "testing(md, 'Ant-v3', 20)\n",
    "testing(md2, 'Ant-v3', 20)\n",
    "testing(m1, 'Ant-v3', 40)\n",
    "testing(md, 'Ant-v3', 40)\n",
    "testing(md2, 'Ant-v3', 40)\n",
    "testing(m1, 'Ant-v3', 80)\n",
    "testing(md, 'Ant-v3', 80)\n",
    "testing(md2, 'Ant-v3', 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lclan/anaconda3/envs/rob_sac/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5741.57 5819.51 376.24 371.54 dis:  0.01 0.01 0.06 0.08\n",
      "10 5730.79 5671.04 366.41 360.79 dis:  0.01 0.01 0.06 0.07\n",
      "10 5763.54 5307.02 368.64 363.66 dis:  0.01 0.01 0.06 0.07\n",
      "20 5962.47 5620.17 378.11 369.14 dis:  0.02 0.02 0.07 0.08\n",
      "20 5583.43 5698.25 368.21 357.66 dis:  0.02 0.02 0.06 0.08\n",
      "20 5596.06 5479.15 369.01 359.34 dis:  0.02 0.02 0.06 0.08\n",
      "40 5625.67 5758.66 377.52 359.95 dis:  0.03 0.03 0.07 0.08\n",
      "40 5675.47 5865.46 368.83 345.89 dis:  0.03 0.03 0.07 0.08\n",
      "40 5571.88 5956.84 368.87 351.24 dis:  0.03 0.03 0.07 0.08\n",
      "80 5796.22 5876.63 376.36 342.60 dis:  0.05 0.06 0.08 0.09\n",
      "80 5770.99 5894.51 366.82 325.21 dis:  0.05 0.06 0.08 0.09\n",
      "80 5706.64 5815.35 366.58 334.24 dis:  0.05 0.06 0.08 0.10\n"
     ]
    }
   ],
   "source": [
    "def change_env_to_adv(env, md, max_eps_t, init_state):\n",
    "    qpos = deepcopy(env.sim.data.qpos)\n",
    "    qvel = deepcopy(env.sim.data.qvel)\n",
    "    grad_pos, grad_vel = get_grad(env, md, max_eps_t)\n",
    "    grad_pos = np.clip(grad_pos, -1, 1)\n",
    "    grad_vel = np.clip(grad_vel, -1, 1)\n",
    "    # adv_qpos = qpos + np.random.uniform(\n",
    "    #         low=-max_eps_t, high=+max_eps_t, size=env.model.nq)\n",
    "    # adv_qvel = qvel + np.random.uniform(\n",
    "    #         low=-max_eps_t, high=+max_eps_t, size=env.model.nv)\n",
    "    adv_qpos = qpos - max_eps_t*grad_pos\n",
    "    adv_qvel = qvel - max_eps_t*grad_vel\n",
    "    init_qpos = init_state[0]\n",
    "    init_qvel = init_state[1]\n",
    "    # rns = env._reset_noise_scale\n",
    "    rns = 0.1\n",
    "    adv_qpos = np.clip(adv_qpos, init_qpos - rns,  init_qpos + rns)\n",
    "    adv_qvel = np.clip(adv_qvel, init_qvel - 2*rns, init_qvel + 2*rns)\n",
    "    \n",
    "    env.set_state(adv_qpos, adv_qvel)\n",
    "    new_obs = env.get_obs()\n",
    "    return new_obs\n",
    "\n",
    "def testing(md, env_name, adv_step, max_eps_t = 0.001):\n",
    "    test_num = 50\n",
    "    test_l = 1000\n",
    "    env = gym.make(env_name)\n",
    "    orig_rs = []\n",
    "    adv_rs = []\n",
    "    orig_qs = []\n",
    "    adv_qs = []\n",
    "    l1_old, l2_old, l1_init, l2_init = [], [], [], []\n",
    "    init_state = get_init_state(env)\n",
    "    for i in range(test_num):\n",
    "        o = env.reset()\n",
    "        old_state = save_state(env)\n",
    "        with torch.torch.no_grad():\n",
    "            q = get_value(o, md).numpy()\n",
    "        orig_qs.append(q)\n",
    "        _, r = run_extra_steps(env, o, 0, md, 1000, test_l)\n",
    "        orig_rs.append(r)\n",
    "        \n",
    "        restore_state(env, old_state)\n",
    "        for _ in range(adv_step):\n",
    "            o = change_env_to_adv(env, md, max_eps_t, init_state)\n",
    "        new_state = save_state(env)\n",
    "        l1, l2 = get_state_dis(new_state, old_state)\n",
    "        l1_old.append(l1)\n",
    "        l2_old.append(l2)\n",
    "        l1, l2 = get_state_dis(new_state, init_state)\n",
    "        l1_init.append(l1)\n",
    "        l2_init.append(l2)\n",
    "        with torch.torch.no_grad():\n",
    "            q = get_value(o, md).numpy()\n",
    "        adv_qs.append(q)\n",
    "        _, r = run_extra_steps(env, o, 0, md, 1000, test_l)\n",
    "        adv_rs.append(r)\n",
    "    print_2f(adv_step, get_mean(orig_rs), get_mean(adv_rs), get_mean(orig_qs), get_mean(adv_qs), \"dis: \", get_mean(l1_old), get_mean(l2_old), get_mean(l1_init), get_mean(l2_init)) \n",
    "\n",
    "testing(m1, 'Ant-v3', 10)\n",
    "testing(md, 'Ant-v3', 10)\n",
    "testing(md2, 'Ant-v3', 10)\n",
    "testing(m1, 'Ant-v3', 20)\n",
    "testing(md, 'Ant-v3', 20)\n",
    "testing(md2, 'Ant-v3', 20)\n",
    "testing(m1, 'Ant-v3', 40)\n",
    "testing(md, 'Ant-v3', 40)\n",
    "testing(md2, 'Ant-v3', 40)\n",
    "testing(m1, 'Ant-v3', 80)\n",
    "testing(md, 'Ant-v3', 80)\n",
    "testing(md2, 'Ant-v3', 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lclan/anaconda3/envs/rob_sac/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5786.07 5772.12 377.05 318.39 dis:  0.07 0.08 0.09 0.11\n",
      "10 5859.89 5729.60 369.58 281.53 dis:  0.07 0.08 0.09 0.11\n",
      "10 5716.49 5603.95 366.56 290.38 dis:  0.07 0.08 0.09 0.11\n",
      "20 5900.58 4438.72 377.43 199.87 dis:  0.11 0.13 0.13 0.16\n",
      "20 5823.18 3897.10 365.89 2.83 dis:  0.12 0.14 0.14 0.16\n",
      "20 5695.11 2645.04 368.51 32.29 dis:  0.12 0.14 0.14 0.17\n",
      "40 5687.60 2130.66 376.84 -59.62 dis:  0.17 0.21 0.19 0.23\n",
      "40 5864.40 1066.36 369.36 -190.44 dis:  0.17 0.21 0.19 0.23\n",
      "40 5796.03 454.48 366.60 -298.71 dis:  0.19 0.23 0.21 0.25\n",
      "80 5654.35 483.88 378.30 -341.87 dis:  0.29 0.35 0.30 0.36\n",
      "80 5677.56 224.40 368.66 -460.74 dis:  0.29 0.36 0.30 0.37\n",
      "80 5699.11 -15.14 368.10 -762.29 dis:  0.36 0.44 0.38 0.45\n"
     ]
    }
   ],
   "source": [
    "def change_env_to_adv(env, md, max_eps_t, init_state):\n",
    "    qpos = deepcopy(env.sim.data.qpos)\n",
    "    qvel = deepcopy(env.sim.data.qvel)\n",
    "    grad_pos, grad_vel = get_grad(env, md, max_eps_t*0.1)\n",
    "    grad_pos = np.clip(grad_pos, -1, 1)\n",
    "    grad_vel = np.clip(grad_vel, -1, 1)\n",
    "    # adv_qpos = qpos + np.random.uniform(\n",
    "    #         low=-max_eps_t, high=+max_eps_t, size=env.model.nq)\n",
    "    # adv_qvel = qvel + np.random.uniform(\n",
    "    #         low=-max_eps_t, high=+max_eps_t, size=env.model.nv)\n",
    "    adv_qpos = qpos - max_eps_t*grad_pos\n",
    "    adv_qvel = qvel - max_eps_t*grad_vel\n",
    "    init_qpos = init_state[0]\n",
    "    init_qvel = init_state[1]\n",
    "    # rns = env._reset_noise_scale\n",
    "    rns = 0.1\n",
    "    # adv_qpos = np.clip(adv_qpos, init_qpos - rns,  init_qpos + rns)\n",
    "    # adv_qvel = np.clip(adv_qvel, init_qvel - 2*rns, init_qvel + 2*rns)\n",
    "    \n",
    "    env.set_state(adv_qpos, adv_qvel)\n",
    "    new_obs = env.get_obs()\n",
    "    return new_obs\n",
    "\n",
    "def testing(md, env_name, adv_step, max_eps_t = 0.01):\n",
    "    test_num = 50\n",
    "    test_l = 1000\n",
    "    env = gym.make(env_name)\n",
    "    orig_rs = []\n",
    "    adv_rs = []\n",
    "    orig_qs = []\n",
    "    adv_qs = []\n",
    "    l1_old, l2_old, l1_init, l2_init = [], [], [], []\n",
    "    init_state = get_init_state(env)\n",
    "    for i in range(test_num):\n",
    "        o = env.reset()\n",
    "        old_state = save_state(env)\n",
    "        with torch.torch.no_grad():\n",
    "            q = get_value(o, md).numpy()\n",
    "        orig_qs.append(q)\n",
    "        _, r = run_extra_steps(env, o, 0, md, 1000, test_l)\n",
    "        orig_rs.append(r)\n",
    "        \n",
    "        restore_state(env, old_state)\n",
    "        for _ in range(adv_step):\n",
    "            o = change_env_to_adv(env, md, max_eps_t, init_state)\n",
    "        new_state = save_state(env)\n",
    "        l1, l2 = get_state_dis(new_state, old_state)\n",
    "        l1_old.append(l1)\n",
    "        l2_old.append(l2)\n",
    "        l1, l2 = get_state_dis(new_state, init_state)\n",
    "        l1_init.append(l1)\n",
    "        l2_init.append(l2)\n",
    "        with torch.torch.no_grad():\n",
    "            q = get_value(o, md).numpy()\n",
    "        adv_qs.append(q)\n",
    "        _, r = run_extra_steps(env, o, 0, md, 1000, test_l)\n",
    "        adv_rs.append(r)\n",
    "    print_2f(adv_step, get_mean(orig_rs), get_mean(adv_rs), get_mean(orig_qs), get_mean(adv_qs), \"dis: \", get_mean(l1_old), get_mean(l2_old), get_mean(l1_init), get_mean(l2_init)) \n",
    "\n",
    "testing(m1, 'Ant-v3', 10)\n",
    "testing(md, 'Ant-v3', 10)\n",
    "testing(md2, 'Ant-v3', 10)\n",
    "testing(m1, 'Ant-v3', 20)\n",
    "testing(md, 'Ant-v3', 20)\n",
    "testing(md2, 'Ant-v3', 20)\n",
    "testing(m1, 'Ant-v3', 40)\n",
    "testing(md, 'Ant-v3', 40)\n",
    "testing(md2, 'Ant-v3', 40)\n",
    "testing(m1, 'Ant-v3', 80)\n",
    "testing(md, 'Ant-v3', 80)\n",
    "testing(md2, 'Ant-v3', 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5871.65 5634.48 378.25 373.21\n",
      "10 5672.27 5641.96 370.84 365.03\n",
      "10 5829.11 5787.20 365.21 360.05\n",
      "20 5823.83 5808.04 376.00 365.68\n",
      "20 5552.02 5774.97 366.45 353.82\n",
      "20 5715.22 5817.36 368.74 358.97\n",
      "40 5938.89 5716.21 376.47 354.42\n",
      "40 5978.07 5540.93 368.98 343.33\n",
      "40 5730.65 5932.85 368.27 346.69\n",
      "80 5702.57 5631.14 378.29 330.14\n",
      "80 5570.50 5729.76 368.67 300.92\n",
      "80 5682.40 5905.85 367.42 318.93\n"
     ]
    }
   ],
   "source": [
    "def get_action(o, md):\n",
    "    with torch.no_grad():\n",
    "        o = torch.as_tensor(o, dtype=torch.float32)\n",
    "        a = md.act(o)\n",
    "    return a\n",
    "\n",
    "\n",
    "def change_env_to_adv(env, md, max_eps_t):\n",
    "    qpos = deepcopy(env.sim.data.qpos)\n",
    "    qvel = deepcopy(env.sim.data.qvel)\n",
    "    grad_pos, grad_vel = get_grad(env, md, max_eps_t)\n",
    "    grad_pos = np.clip(grad_pos, -1, 1)\n",
    "    grad_vel = np.clip(grad_vel, -1, 1)\n",
    "    # adv_qpos = qpos + np.random.uniform(\n",
    "    #         low=-max_eps_t, high=+max_eps_t, size=env.model.nq)\n",
    "    # adv_qvel = qvel + np.random.uniform(\n",
    "    #         low=-max_eps_t, high=+max_eps_t, size=env.model.nv)\n",
    "    adv_qpos = qpos - max_eps_t*grad_pos\n",
    "    adv_qvel = qvel - max_eps_t*grad_vel\n",
    "    init_qpos = env.sim.data.qpos.ravel().copy()\n",
    "    init_qvel = env.sim.data.qvel.ravel().copy()\n",
    "    # rns = env._reset_noise_scale\n",
    "    rns = 0.1\n",
    "    adv_qpos = np.clip(adv_qpos, init_qpos - rns,  init_qpos + rns)\n",
    "    adv_qvel = np.clip(adv_qvel, init_qvel - 2*rns, init_qvel + 2*rns)\n",
    "    \n",
    "    env.set_state(adv_qpos, adv_qvel)\n",
    "    new_obs = env.get_obs()\n",
    "    return new_obs\n",
    "\n",
    "def testing(md, env_name, adv_step, max_eps_t = 0.001):\n",
    "    test_num = 50\n",
    "    test_l = 1000\n",
    "    env = gym.make(env_name)\n",
    "    orig_rs = []\n",
    "    adv_rs = []\n",
    "    orig_qs = []\n",
    "    adv_qs = []\n",
    "    for i in range(test_num):\n",
    "        o = env.reset()\n",
    "        old_state = save_state(env)\n",
    "        with torch.torch.no_grad():\n",
    "            q = get_value(o, md).numpy()\n",
    "        orig_qs.append(q)\n",
    "        _, r = run_extra_steps(env, o, 0, md, 1000, test_l)\n",
    "        orig_rs.append(r)\n",
    "        \n",
    "        restore_state(env, old_state)\n",
    "        for _ in range(adv_step):\n",
    "            o = change_env_to_adv(env, md, max_eps_t)\n",
    "        \n",
    "        with torch.torch.no_grad():\n",
    "            q = get_value(o, md).numpy()\n",
    "        adv_qs.append(q)\n",
    "        _, r = run_extra_steps(env, o, 0, md, 1000, test_l)\n",
    "        adv_rs.append(r)\n",
    "    print_2f(adv_step, get_mean(orig_rs), get_mean(adv_rs), get_mean(orig_qs), get_mean(adv_qs))\n",
    "    \n",
    "\n",
    "testing(m1, 'Ant-v3', 10)\n",
    "testing(md, 'Ant-v3', 10)\n",
    "testing(md2, 'Ant-v3', 10)\n",
    "testing(m1, 'Ant-v3', 20)\n",
    "testing(md, 'Ant-v3', 20)\n",
    "testing(md2, 'Ant-v3', 20)\n",
    "testing(m1, 'Ant-v3', 40)\n",
    "testing(md, 'Ant-v3', 40)\n",
    "testing(md2, 'Ant-v3', 40)\n",
    "testing(m1, 'Ant-v3', 80)\n",
    "testing(md, 'Ant-v3', 80)\n",
    "testing(md2, 'Ant-v3', 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.randint(0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MjSimState(time=0.0, qpos=array([-0.02617342, -0.02761698,  0.75605344,  0.99485032, -0.03005331,\n",
      "        0.03483065,  0.09031313,  0.04466091, -0.03153617,  0.06298635,\n",
      "        0.06318186, -0.09845077, -0.02295335, -0.02311682, -0.01486309]), qvel=array([ 0.04955178, -0.04639389, -0.0674654 ,  0.03095215, -0.01689994,\n",
      "        0.1368381 ,  0.07399187, -0.07890817, -0.01606276,  0.1702014 ,\n",
      "        0.05325084, -0.13882166,  0.03134442, -0.00683104]), act=None, udd_state={})\n",
      "MjSimState(time=0.0, qpos=array([-0.02617342, -0.02761698,  0.75605344,  0.99485032, -0.03005331,\n",
      "        0.03483065,  0.09031313,  0.04466091, -0.03153617,  0.06298635,\n",
      "        0.06318186, -0.09845077, -0.02295335, -0.02311682, -0.01486309]), qvel=array([ 0.04955178, -0.04639389, -0.0674654 ,  0.03095215, -0.01689994,\n",
      "        0.1368381 ,  0.07399187, -0.07890817, -0.01606276,  0.1702014 ,\n",
      "        0.05325084, -0.13882166,  0.03134442, -0.00683104]), act=None, udd_state={})\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "s = env.sim.get_state()\n",
    "print(s)\n",
    "env.sim.forward()\n",
    "s = env.sim.get_state()\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MjSimState(time=23.200000000000827, qpos=array([ 1.34553116e+02, -1.74787395e+01,  5.57658190e-01,  8.90495813e-01,\n",
      "       -2.39190275e-02, -2.73955727e-02,  4.53535632e-01, -5.26442403e-01,\n",
      "        5.21601129e-01,  6.27414610e-02, -5.22426270e-01,  5.29494749e-01,\n",
      "       -5.22962011e-01, -1.48080803e-01,  5.21909865e-01]), qvel=array([ 4.97041445e+00, -1.08096313e+00, -1.60077800e+00,  2.88261376e-01,\n",
      "       -8.11529663e-01, -3.22964140e-01,  6.00473944e-02, -5.30573276e-03,\n",
      "        8.61205332e+00,  5.24909571e-03, -1.42019404e-01, -2.34475657e-02,\n",
      "        7.51371280e-01, -4.53819375e-04]), act=None, udd_state={})\n"
     ]
    }
   ],
   "source": [
    "o = env.get_obs()\n",
    "a = get_action(o, md)\n",
    "env.step(a)\n",
    "s2 = env.sim.get_state()\n",
    "print(s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MjSimState(time=23.15000000000082, qpos=array([ 1.34300706e+02, -1.74218079e+01,  6.25926612e-01,  8.95516314e-01,\n",
      "       -3.69560323e-02, -1.06297132e-02,  4.43364175e-01, -5.35470623e-01,\n",
      "        5.21944544e-01, -2.29842399e-01, -5.22659800e-01,  5.50305118e-01,\n",
      "       -5.20308676e-01, -8.98953268e-02,  5.21159316e-01]), qvel=array([ 5.10619435e+00, -1.20159320e+00, -1.12681048e+00,  5.72246202e-02,\n",
      "       -8.59304756e-01,  1.19870253e+00,  3.83812317e-01,  2.02744936e-03,\n",
      "        3.04500041e+00, -1.12547319e-02, -8.65269143e-01, -8.01163914e-02,\n",
      "       -3.11102177e+00,  6.19502554e-02]), act=None, udd_state={})\n"
     ]
    }
   ],
   "source": [
    "env.sim.set_state(s)\n",
    "env.sim.forward()\n",
    "s = env.sim.get_state()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "l = [1,2,3]\n",
    "l2 = [1,2,3]\n",
    "l3 = l+l2\n",
    "print(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 2, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "l3.sort()\n",
    "print(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "md = torch.load('./ppo-ant.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pi', 'obs_mean', 'obs_std', 'clip'])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.container.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "class Actor():\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        self.pi = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
    "        self.obs_mean = np.ones(obs_dim)\n",
    "        self.obs_std = np.ones(obs_dim)\n",
    "        self.clip = 10.0\n",
    "        print(type(self.pi))\n",
    "    \n",
    "    def normalize_o(self, o):\n",
    "        o = o - self.obs_mean\n",
    "        o = o / (self.obs_std + 1e-8)\n",
    "        o = np.clip(o, -self.clip, self.clip)\n",
    "        return o\n",
    "    \n",
    "    def act(self, o):\n",
    "        if torch.is_tensor(o):\n",
    "            o = o.numpy()\n",
    "        o = self.normalize_o(o)\n",
    "        o = torch.as_tensor(o, dtype=torch.float32)\n",
    "        return self.pi(o).detach().numpy()\n",
    "    \n",
    "    def copy_model(self, md):\n",
    "        self.pi.load_state_dict(md['pi'])\n",
    "        self.obs_mean = md['obs_mean']\n",
    "        self.obs_std = md['obs_std']\n",
    "        self.clip = md['clip']\n",
    "\n",
    "x = Actor(111, 8, (64, 64), nn.Tanh)\n",
    "md = torch.load('./ppo-ant.pt')\n",
    "x.copy_model(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5857.734883010637\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from copy import deepcopy \n",
    "env = gym.make('Ant-v3')\n",
    "\n",
    "def get_action(o, md):\n",
    "    o = torch.as_tensor(o, dtype=torch.float32)\n",
    "    return md.act(o)\n",
    "\n",
    "def test_model(env, model, max_ep_len=1000, num_episodes=5):\n",
    "    o, r, d, ep_ret, ep_len, n = env.reset(), 0, False, 0, 0, 0\n",
    "    total_rewards = []\n",
    "    trajs = []\n",
    "    mid_points = []\n",
    "    while n < num_episodes:\n",
    "        old_state = deepcopy(env.sim.get_state())\n",
    "        mid_points.append((ep_len, old_state, o))\n",
    "        a = get_action(o, model)\n",
    "        o, r, d, _ = env.step(a)\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            total_rewards.append(ep_ret)\n",
    "            trajs.append(mid_points)\n",
    "            mid_points = [] # clear mid point\n",
    "            o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "            n += 1\n",
    "    return total_rewards, trajs\n",
    "\n",
    "rs ,tjs = test_model(env, x)\n",
    "\n",
    "def vmean(v):\n",
    "    return sum(v) / len(v)\n",
    "\n",
    "print(vmean(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Ant-v3')\n",
    "print(env.observation_space.shape[0])\n",
    "print(env.action_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23\t23.12\t2321.23\n"
     ]
    }
   ],
   "source": [
    "def print_2f_tab(*args):\n",
    "    __builtins__.print(*(\"%.2f\" % a if isinstance(a, float) else a\n",
    "                         for a in args), sep='\\t')\n",
    "\n",
    "a = [1.233543, 23.12313412, 2321.23]\n",
    "print_2f_tab(*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d07456bc64aaa948bb07d8fc241a93772a319883e604e16666e0b6e2246aebd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('rob_sac': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
