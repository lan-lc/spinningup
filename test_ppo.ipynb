{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import mod\n",
    "from telnetlib import DM\n",
    "from turtle import mode\n",
    "from unicodedata import name\n",
    "import gym\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "from scipy import stats\n",
    "from statistics import mean \n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "import itertools\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class PPO_Actor():\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        self.pi = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
    "        self.obs_mean = np.ones(obs_dim)\n",
    "        self.obs_std = np.ones(obs_dim)\n",
    "        self.clip = 10.0\n",
    "        # print(type(self.pi))\n",
    "    \n",
    "    def normalize_o(self, o):\n",
    "        o = o - self.obs_mean\n",
    "        o = o / (self.obs_std + 1e-8)\n",
    "        o = np.clip(o, -self.clip, self.clip)\n",
    "        return o\n",
    "    \n",
    "    def act(self, o):\n",
    "        if torch.is_tensor(o):\n",
    "            o = o.numpy()\n",
    "        o = self.normalize_o(o)\n",
    "        o = torch.as_tensor(o, dtype=torch.float32)\n",
    "        return self.pi(o).detach().numpy()\n",
    "    \n",
    "    def copy_model(self, md):\n",
    "        self.pi.load_state_dict(md['pi'])\n",
    "        self.obs_mean = md['obs_mean']\n",
    "        self.obs_std = md['obs_std']\n",
    "        self.clip = md['clip'] \n",
    "        \n",
    "    def load(self, name):\n",
    "        md = torch.load(name)\n",
    "        self.copy_model(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lclan/anaconda3/envs/rob_sac/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "def get_env_name(name):\n",
    "    if 'humanoid' in name:\n",
    "        return 'Humanoid-v2'\n",
    "    if 'halfcheetah' in name:\n",
    "        return 'HalfCheetah-v2'\n",
    "    if 'ant' in name:\n",
    "        return 'Ant-v2'\n",
    "    elif 'hopper' in name:\n",
    "        return 'Hopper-v2'\n",
    "    elif 'walker' in name:\n",
    "        return 'Walker2d-v2'\n",
    "    return 'unknown'\n",
    "name ='./data/vanilla_ppo_humanoid/vanilla_ppo_humanoid_0.pt'\n",
    "env_name = get_env_name(name)\n",
    "env = gym.make(env_name)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "x = PPO_Actor(obs_dim, action_dim, (64, 64), nn.Tanh)\n",
    "x.load(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(o, md):\n",
    "    o = torch.as_tensor(o, dtype=torch.float32)\n",
    "    a = md.act(o)\n",
    "    return a\n",
    "def test_model(env, model, num_episodes=20):\n",
    "    max_ep_len = 1000\n",
    "    o, r, d, ep_ret, ep_len, n = env.reset(), 0, False, 0, 0, 0\n",
    "    total_rewards = []\n",
    "    trajs = []\n",
    "    while n < num_episodes:\n",
    "        a = get_action(o, model)\n",
    "        o, r, d, _ = env.step(a)\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            total_rewards.append(ep_ret)\n",
    "            trajs.append(mid_points)\n",
    "            mid_points = {}\n",
    "            o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "            n += 1\n",
    "    return total_rewards, trajs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d07456bc64aaa948bb07d8fc241a93772a319883e604e16666e0b6e2246aebd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('rob_sac': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
